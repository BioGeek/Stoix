# --- Defaults FF-MPO ---

total_timesteps: 1e8 # Set the total environment steps.
# If unspecified, it's derived from num_updates; otherwise, num_updates adjusts based on this value.
num_updates: ~ # Number of updates
seed: 0

# --- RL hyperparameters ---
update_batch_size: 1 # Number of vectorised gradient updates per device.
rollout_length: 5 # Number of environment steps per vectorised environment.
epochs: 16 # Number of sgd steps per rollout.
warmup_steps: 256  # Number of steps to collect before training.
buffer_size: 100_000 # size of the replay buffer.
batch_size: 256 # Number of samples to train on per device.
actor_lr: 1e-4  # the learning rate of the policy network optimizer
q_lr: 1e-4  # the learning rate of the Q network network optimizer
dual_lr: 1e-2  # the learning rate of the alpha optimizer
tau: 0.005  # smoothing coefficient for target networks
gamma: 0.99  # discount factor
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
max_abs_reward : 20_000  # maximum absolute reward value
huber_loss_parameter : 1.0 # Huber loss parameter for Q-value regression.
num_samples: 20 # Number of MPO action samples.
epsilon: 0.1 # KL constraint on the non-parametric auxiliary policy, the one associated with the dual variable called temperature.
epsilon_mean : 0.0025 # KL constraint on the mean of the Gaussian policy, the one associated with the dual variable called alpha_mean.
epsilon_stddev: 1e-6 # KL constraint on the stddev of the Gaussian policy, the one associated with the dual variable called alpha_mean.
init_log_temperature: 10. # initial value for the temperature in log-space, note a softplus (rather than an exp) will be used to transform this.
init_log_alpha_mean: 10. # initial value for the alpha_mean in log-space, note a softplus (rather than an exp) will be used to transform this.
init_log_alpha_stddev: 1000. # initial value for the alpha_stddev in log-space, note a softplus (rather than an exp) will be used to transform this.
per_dim_constraining: True # whether to enforce the KL constraint on each dimension independently; this is the default. Otherwise the overall KL is constrained, which allows some dimensions to change more at the expense of others staying put.
action_penalization: True # whether to use a KL constraint to penalize actions via the MO-MPO algorithm.
epsilon_penalty: 0.001 # KL constraint on the probability of violating the action constraint.
