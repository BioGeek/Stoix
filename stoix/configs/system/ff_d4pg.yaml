# --- Defaults FF-D4PG ---

system_name: ff_d4pg # Name of the system.

# --- RL hyperparameters ---
update_batch_size: 1 # Number of vectorised gradient updates per device.
rollout_length: 8 # Number of environment steps per vectorised environment.
epochs: 32 # Number of sgd steps per rollout.
warmup_steps: 256  # Number of steps to collect before training.
buffer_size: 500_000 # size of the replay buffer.
batch_size: 256 # Number of samples to train on per device.
actor_lr: 3e-4  # the learning rate of the policy network optimizer
q_lr: 3e-4  # the learning rate of the Q network network optimizer
tau: 0.01  # smoothing coefficient for target networks
gamma: 0.99  # discount factor
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
max_abs_reward : 20_000  # maximum absolute reward value
num_atoms: 301  # number of atoms in the distributional Q network
v_min: -9_000.0  # minimum value of the support
v_max: 9_000.0  # maximum value of the support
exploration_sigma : 0.1  # standard deviation of the exploration noise
