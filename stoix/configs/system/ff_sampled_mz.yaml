# --- Defaults FF-Sampled-MZ ---
# This implementation of Sampled MuZero is not an exact replica of the original Sampled MuZero algorithm.
# It is a simplified version that uses a feed forward network for the representation function and does not use observation
# history. It also does not do tiling and encoding of actions in a 2D plane. A non-priority buffer is used as well.
# Additionally, the search method used can be chosen between muzero mcts and gumbel mcts from mctx. Lastly, this version
# is designed with a Gaussian policy in mind.

system_name: ff_sampled_mz # Name of the system.

# --- RL hyperparameters ---
lr: 3e-4 # Learning rate for entire algorithm.
update_batch_size: 1 # Number of vectorised gradient updates per device.
rollout_length: 8 # Number of environment steps per vectorised environment.
epochs: 8 # Number of epochs per training data batch.
warmup_steps: 8 # Number of steps to collect before training.
buffer_size: 500 # Size of the replay buffer.
batch_size: 16 # Number of samples to train on per device.
sample_sequence_length: 8 # Number of steps to consider for each element of the batch.
period : 1 # Period of the sampled sequences.
gamma: 0.99 # Discounting factor.
n_steps: 5 # Number of steps to use for bootstrapped returns.
ent_coef: 0.005 # Entropy regularisation term for loss function.
vf_coef: 0.25 # Critic weight in
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
num_simulations: 20 # Number of simulations to run.
max_depth: ~ # Maximum depth of the search tree.
search_method : muzero # Search method to use. Options: gumbel, muzero.
search_method_kwargs: {} # Additional kwargs for the search method.
critic_vmin: -150.0 # Minimum value for the critic.
critic_vmax: 150.0 # Maximum value for the critic.
critic_num_atoms: 51 # Number of atoms for the categorical critic head.
reward_vmin: -10.0 # Minimum value for the reward.
reward_vmax: 10.0 # Maximum value for the reward.
reward_num_atoms: 51 # Number of atoms for the categorical reward head.
num_samples: 10 # Number of action samples to use in search.
root_exploration_sigma : 0.05 # Noise to add to the root node sampled actions.
