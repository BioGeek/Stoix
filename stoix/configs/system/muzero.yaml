# --- Defaults FF-AZ ---
# This implementation of AZ is not an exact replica of the original AlphaZero algorithm and serves more as an example.
# It is a simplified version that uses the same training loop as PPO i.e. epochs and minibatches of on-policy data.
# Additionally, it uses a clipped value loss and entropy loss. Additionally, the search method used can be chosen
# between muzero mcts, gumbel mcts from mctx.

system_name: muzero # Name of the system.

# --- RL hyperparameters ---
actor_lr: 3e-4 # Learning rate for actor network
critic_lr: 3e-4 # Learning rate for critic network
world_model_lr: 3e-4 # Learning rate for world model networks
update_batch_size: 1 # Number of vectorised gradient updates per device.
rollout_length: 8 # Number of environment steps per vectorised environment.
epochs: 1 # Number of epochs per training data batch.
warmup_steps: 64 # Number of steps to collect before training.
buffer_size: 10_000 # Size of the replay buffer.
batch_size: 16 # Number of samples to train on per device.
sample_sequence_length: 50 # Number of steps to consider for each element of the batch.
period : 1 # Period of the sampled sequences.
gamma: 0.99 # Discounting factor.
n_steps: 5 # Number of steps to use for bootstrapped returns.
ent_coef: 0.0 # Entropy regularisation term for loss function.
vf_coef: 1.0 # Critic weight in
max_grad_norm: 0.5 # Maximum norm of the gradients for a weight update.
decay_learning_rates: False # Whether learning rates should be linearly decayed during training.
num_simulations: 10 # Number of simulations to run.
max_depth: ~ # Maximum depth of the search tree.
search_method : gumbel # Search method to use. Options: gumbel, muzero.
